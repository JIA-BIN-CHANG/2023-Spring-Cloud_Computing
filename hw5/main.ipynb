{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7901552-72c6-48cd-a63d-92492d0714c8",
   "metadata": {},
   "source": [
    "# Download data needed (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f72f84-ff77-4cdd-82d7-68befc24bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "files = '''\n",
    "2010/01052099999.csv\n",
    "2010/99407099999.csv\n",
    "2011/01008099999.csv\n",
    "2011/01046099999.csv\n",
    "2012/01023099999.csv\n",
    "2012/01044099999.csv\n",
    "2013/01001499999.csv\n",
    "2013/01008099999.csv\n",
    "2014/01008099999.csv\n",
    "2014/01023099999.csv\n",
    "2015/01008099999.csv\n",
    "2015/01025099999.csv\n",
    "2016/01008099999.csv\n",
    "2016/01023199999.csv\n",
    "2017/01008099999.csv\n",
    "2017/01023099999.csv\n",
    "2018/01008099999.csv\n",
    "2018/01025099999.csv\n",
    "2019/01008099999.csv\n",
    "2019/01023099999.csv\n",
    "2020/01008099999.csv\n",
    "2020/01023099999.csv\n",
    "2021/01062099999.csv\n",
    "2021/01065099999.csv\n",
    "2022/01241099999.csv\n",
    "2022/02095099999.csv\n",
    "'''\n",
    "files = files.split()\n",
    "print(files)\n",
    "\n",
    "from os import mkdir\n",
    "from os.path import isdir\n",
    "if not isdir('data'):\n",
    "    mkdir('data')\n",
    "for f in files:\n",
    "    folder, file = f.split('/')\n",
    "    if not isdir(f'data/{folder}'):\n",
    "        mkdir(f'data/{folder}')\n",
    "    else:\n",
    "        urllib.request.urlretrieve(f\"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/{f}\", \n",
    "                                f'data/{folder}/{file}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54adf8c1-8af0-4766-a463-7d28e806675c",
   "metadata": {},
   "source": [
    "# Setup and initialize spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e578269d-81d5-410d-b9d8-5d63f16cd122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/29 15:45:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import size, year, month\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce21bda5-7500-4071-99d0-181360f3ec5e",
   "metadata": {},
   "source": [
    "# Read data and create year month data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa1dbe6-2854-4186-b1b6-f682a6fadd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.option(\"header\", \"true\").option(\"delimiter\",\",\").option(\"inferSchema\", \"true\").csv('data/*')\n",
    "spark_df = spark_df.withColumn('YEAR', year(spark_df.DATE))\n",
    "spark_df = spark_df.withColumn('MONTH', month(spark_df.DATE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e632952-72f3-43d9-a15b-002385b41c99",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0b3a019-03ba-4908-a322-5d821436ece4",
   "metadata": {},
   "source": [
    "Get rid of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec78b9d1-e7e3-46e3-adba-d6ccb340d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/29 15:45:53 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "spark_df_Hot = spark_df.filter(spark_df.MAX != 9999.9)\n",
    "spark_df_Hot.createOrReplaceTempView('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dfc6991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+--------------------+-------------------+\n",
      "| MAX|    STATION|                NAME|               DATE|\n",
      "+----+-----------+--------------------+-------------------+\n",
      "|74.8|99407099999|DESTRUCTION IS. W...|2010-08-15 00:00:00|\n",
      "|87.8| 1046099999|       SORKJOSEN, NO|2011-07-09 00:00:00|\n",
      "|72.0| 1023099999|       BARDUFOSS, NO|2012-07-05 00:00:00|\n",
      "|80.6| 1001499999|      SORSTOKKEN, NO|2013-08-02 00:00:00|\n",
      "|89.6| 1023099999|       BARDUFOSS, NO|2014-07-10 00:00:00|\n",
      "|71.6| 1025099999|          TROMSO, NO|2015-07-30 00:00:00|\n",
      "|77.0| 1023199999|         DRAUGEN, NO|2016-07-21 00:00:00|\n",
      "|78.6| 1023099999|       BARDUFOSS, NO|2017-06-09 00:00:00|\n",
      "|84.2| 1025099999|          TROMSO, NO|2018-07-29 00:00:00|\n",
      "|78.8| 1023099999|       BARDUFOSS, NO|2019-07-21 00:00:00|\n",
      "|79.9| 1023099999|       BARDUFOSS, NO|2020-06-22 00:00:00|\n",
      "|88.3| 1065099999|        KARASJOK, NO|2021-07-05 00:00:00|\n",
      "|85.5| 2095099999|          PAJALA, SW|2022-07-01 00:00:00|\n",
      "+----+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT t1.MAX, t1.STATION, t1.NAME, t1.DATE\n",
    "FROM (\n",
    "  SELECT MAX, STATION, NAME, DATE, YEAR,\n",
    "         ROW_NUMBER() OVER (PARTITION BY YEAR ORDER BY MAX DESC) as row\n",
    "  FROM table\n",
    ") t1\n",
    "INNER JOIN (\n",
    "  SELECT YEAR, MAX(MAX) as M_TEMP\n",
    "  FROM table\n",
    "  GROUP BY YEAR\n",
    ") t2\n",
    "ON t1.YEAR = t2.YEAR AND t1.MAX = t2.M_TEMP\n",
    "WHERE t1.row = 1\n",
    "ORDER BY t1.YEAR\n",
    "\n",
    "'''\n",
    "ori_stdout = sys.stdout\n",
    "with open('result.txt', 'a') as f:\n",
    "    sys.stdout = f\n",
    "    print('Task 1')\n",
    "    spark.sql(query).show()\n",
    "    sys.stdout = ori_stdout\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d141f23a-03cf-4b46-bc79-267db50dc6b8",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "533e75d5-4e73-44c3-a567-8f5267c22d00",
   "metadata": {},
   "source": [
    "Get rid of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a78a1e2-f1f7-4ddb-9dad-d1a3ef7139b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_Cold = spark_df.filter(spark_df.MIN != 9999.9)\n",
    "spark_df_Cold.createOrReplaceTempView('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa656e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------+-------------------+\n",
      "|  MIN|   STATION|         NAME|               DATE|\n",
      "+-----+----------+-------------+-------------------+\n",
      "|-28.3|1023099999|BARDUFOSS, NO|2017-01-05 00:00:00|\n",
      "+-----+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT t.MIN, t.STATION, t.NAME, t.DATE\n",
    "FROM table t\n",
    "WHERE t.MONTH = 1 AND t.YEAR BETWEEN 2010 AND 2022 AND t.MIN = (\n",
    "  SELECT MIN(MIN)\n",
    "  FROM table\n",
    "  WHERE MONTH = 1 AND YEAR BETWEEN 2010 AND 2022\n",
    ")\n",
    "'''\n",
    "ori_stdout = sys.stdout\n",
    "with open('result.txt', 'a') as f:\n",
    "    sys.stdout = f\n",
    "    print('Task 2')\n",
    "    spark.sql(query).show()\n",
    "    sys.stdout = ori_stdout\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdcd26f8-fbce-494e-b95c-c2a884eb5a2c",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d9be2b1-3f29-4043-a2f0-d81303b8e03a",
   "metadata": {},
   "source": [
    "Get rid of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10bee138-8b54-4f25-a5d0-174ae8b692e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_PRCP = spark_df.filter(spark_df.PRCP != 99.99)\n",
    "spark_df_PRCP.createOrReplaceTempView('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c193cc-00d1-4e99-92ef-5cfab0033c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------+-------------------+\n",
      "|PRCP|   STATION|        NAME|               DATE|\n",
      "+----+----------+------------+-------------------+\n",
      "|2.11|1025099999|  TROMSO, NO|2015-11-02 00:00:00|\n",
      "| 0.0|1008099999|LONGYEAR, SV|2015-01-01 00:00:00|\n",
      "+----+----------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "(SELECT PRCP, STATION, NAME, DATE\n",
    "FROM table\n",
    "WHERE YEAR = 2015\n",
    "ORDER BY PRCP DESC\n",
    "limit 1)\n",
    "UNION\n",
    "(SELECT PRCP, STATION, NAME, DATE\n",
    "FROM table\n",
    "WHERE YEAR = 2015\n",
    "ORDER BY PRCP ASC\n",
    "limit 1)\n",
    "'''\n",
    "ori_stdout = sys.stdout\n",
    "with open('result.txt', 'a') as f:\n",
    "    sys.stdout = f\n",
    "    print('Task 3')\n",
    "    spark.sql(query).show()\n",
    "    sys.stdout = ori_stdout\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "decc800d-e08a-45f9-af34-15a9457b04c6",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a53ab0e-df02-4c64-a4f2-4e98a34ae1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------------+\n",
      "|n_nan|n_rows|percentage_missing|\n",
      "+-----+------+------------------+\n",
      "|  605|   730| 82.87671232876713|\n",
      "+-----+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.createOrReplaceTempView('table')\n",
    "\n",
    "query = '''\n",
    "SELECT n_nan, n_rows, n_nan / n_rows * 100 as percentage_missing\n",
    "FROM\n",
    "(\n",
    "SELECT COUNT(*) as n_nan\n",
    "FROM table\n",
    "WHERE YEAR = 2019 AND GUST = 999.9\n",
    ")\n",
    ",\n",
    "(\n",
    "SELECT COUNT(*) as n_rows\n",
    "FROM table\n",
    "WHERE YEAR = 2019\n",
    ")\n",
    "'''\n",
    "ori_stdout = sys.stdout\n",
    "with open('result.txt', 'a') as f:\n",
    "    sys.stdout = f\n",
    "    print('Task 4')\n",
    "    spark.sql(query).show()\n",
    "    sys.stdout = ori_stdout\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "843aac53-51cd-482e-b0c4-0dbf52d81216",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "697465a5-1790-4807-8372-a0a4692fb0b2",
   "metadata": {},
   "source": [
    "Get rid of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d6f6c96-56fc-4ae7-9e17-02917753f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_TEMP = spark_df.filter(spark_df.TEMP != 9999.9)\n",
    "spark_df_TEMP.createOrReplaceTempView('table')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bab9fb3b-da1f-425c-b531-d51dd0a69434",
   "metadata": {},
   "source": [
    "Get Mean, Median, and Standard Deviatiion usnig first query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bc466bd-87c5-40ec-af5e-b9675951d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT MONTH, AVG(TEMP) as MEAN, percentile(TEMP, 0.5) as MEDIAN, STD(TEMP) as STD\n",
    "FROM table\n",
    "WHERE YEAR = 2020\n",
    "GROUP BY MONTH\n",
    "ORDER BY MONTH\n",
    "'''\n",
    "\n",
    "spark_df_summary = spark.sql(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ffed7a3-7446-429a-9175-306e0d86ade0",
   "metadata": {},
   "source": [
    "Calculating Mode by manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e839060b-02e1-40c3-84ee-ec728893b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT a.MONTH, b.TEMP\n",
    "FROM\n",
    "(\n",
    "SELECT MONTH, MAX(COUNT) as M_COUNT\n",
    "FROM\n",
    "(\n",
    "SELECT MONTH, TEMP, COUNT(TEMP) as COUNT\n",
    "FROM table\n",
    "WHERE YEAR = 2020\n",
    "GROUP BY TEMP, MONTH\n",
    ")\n",
    "GROUP BY MONTH\n",
    "ORDER BY MONTH\n",
    ") a\n",
    "INNER JOIN\n",
    "(\n",
    "SELECT MONTH, TEMP, COUNT(TEMP) as COUNT\n",
    "FROM table\n",
    "WHERE YEAR = 2020\n",
    "GROUP BY TEMP, MONTH\n",
    ") b\n",
    "ON a.MONTH = b.MONTH AND a.M_COUNT = b.COUNT\n",
    "ORDER BY a.MONTH\n",
    "'''\n",
    "\n",
    "spark.sql(query).createOrReplaceTempView('count')\n",
    "\n",
    "query = '''\n",
    "SELECT MONTH, TEMP as MODE\n",
    "FROM (\n",
    "      SELECT MONTH, TEMP, ROW_NUMBER() OVER (PARTITION BY MONTH ORDER BY TEMP)row_num\n",
    "      FROM count\n",
    "      ) sub\n",
    "WHERE row_num = 1\n",
    "'''\n",
    "spark_df_mode = spark.sql(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41882321-73b5-4bef-95ad-b5cfedf4336b",
   "metadata": {},
   "source": [
    "Combine two table to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5bfd5c3-f2db-428e-aaa9-316dd899d7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------+----+------------------+\n",
      "|MONTH|              MEAN|MEDIAN|MODE|               STD|\n",
      "+-----+------------------+------+----+------------------+\n",
      "|    1|15.896774193548387| 15.25| 5.7|12.805172721989297|\n",
      "|    2| 13.35862068965517|  15.4| 2.8| 13.09180853418292|\n",
      "|    3|14.653225806451612|  18.6| 9.2|15.784789500893567|\n",
      "|    4|23.329999999999995|  27.3|34.1| 13.02209725617009|\n",
      "|    5| 36.21935483870967| 36.05|37.0| 8.077246704851957|\n",
      "|    6| 47.42999999999999|  46.1|36.7| 8.877190347997287|\n",
      "|    7| 52.88709677419356| 51.55|49.3|  6.66378723291517|\n",
      "|    8|49.287096774193564| 48.85|44.7| 6.548594740281946|\n",
      "|    9| 41.84499999999999| 42.55|31.8| 5.887660897797832|\n",
      "|   10|31.529032258064507|  30.9|23.2| 9.609052888228815|\n",
      "|   11|29.246666666666666|  29.9|28.1| 8.143448373534971|\n",
      "|   12| 19.95483870967743| 20.35|10.2| 8.854464048157649|\n",
      "+-----+------------------+------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ori_stdout = sys.stdout\n",
    "with open('result.txt', 'a') as f:\n",
    "    sys.stdout = f\n",
    "    print('Task 5')\n",
    "    spark_df_summary.join(spark_df_mode, 'MONTH', 'outer').select('MONTH', 'MEAN', 'MEDIAN', 'MODE', 'STD').show()\n",
    "    sys.stdout = ori_stdout\n",
    "spark_df_summary.join(spark_df_mode, 'MONTH', 'outer').select('MONTH', 'MEAN', 'MEDIAN', 'MODE', 'STD').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
